CMAESX: results from slides

CMA: results with 3 different objective types (100 steps)
CMA0: results with 3 different objective types (300 steps)
CMA1: results with 3 different objective types (100 steps, random start)
CMA2: results with 3 different objective types (100 steps, random start)

CMA3: results with 10 standard objectives (100 steps, random start, version==1)
RLCMA2: results with 10 standard objectives (100 steps, random start, version==2)

# params cma-es
Namespace(dim=64, eval=False, noise=0.0, obj='fsphere', pop=256, reps=1, run=0, steps=100)
Running with cs: 0.5068372800075271, cm: 1, cu: 0.02933521975901428

Namespace(dim=32, eval=False, noise=0.0, obj='fsphere', pop=128, reps=1, run=0, steps=100)
Running with cs: 0.5082796550309218, cm: 1, cu: 0.05412284473896106

Namespace(dim=16, eval=False, noise=0.0, obj='fsphere', pop=64, reps=1, run=0, steps=100)
Running with cs: 0.5080479778584683, cm: 1, cu: 0.09178809752550958

Namespace(dim=8, eval=False, noise=0.0, obj='fsphere', pop=32, reps=1, run=0, steps=100)
Running with cs: 0.5040315229069345, cm: 1, cu: 0.13350162016265607

Namespace(dim=4, eval=False, noise=0.0, obj='fsphere', pop=16, reps=1, run=0, steps=100)
Running with cs: 0.4942530712443723, cm: 1, cu: 0.14923696382608587

Namespace(dim=2, eval=False, noise=0.0, obj='fsphere', pop=8, reps=1, run=0, steps=100)
Running with cs: 0.479176368423509, cm: 1, cu: 0.10588798830238358

# Run 5
standard setup
running 2,4,8 dimension w 1e7 experiences
Finding: learning stops for 2 dim after ~3e6 exp, no learning for 4 & 8 dim

# Run 6
added mu learning rate as action, removed diagonal cov states, scaling parameter samples by diagoanl sdev
running 2 and 4 dimension
Finding: no improvement detected, ill continue with this varyation as it seems to be more reasonable

# Run 1
Actions Between Policy Updates set to 1 (used to be 10!) Rerunning all objectives wo noise, lets see if the agent learns
Finding: agent learns, except for rosenbrock very flat. rosenbrock with params (100, 5) and spheres (5,5) same perf as cmaes. but worked well on booth, himmelblau and levi, next changing params of rosenbrock and retry (Run 2)
Todo: check runs of levi13 and ackley

# Run 2 (deleted)
using 3 actions, cs, cmu and csaddon. Rerunning rosenbrock and spheres wo noise, lets compare with run 0.
Finding: cov error!!!
Todo: investigate, follow-up

# Run 2
Same as Run 1, but changed params of rosenbrock (100, 3)
Finding: agent still learns flat w pop 8 and 32

# Run 3
Same as Run 2, but increasing population size to 32 s.t. variance of reward reduced  and potentially increases learning. running 'random' wo rosenbrock
Finding: learning seems to be similar, not much improvement when comparing with plain cma-es, random32 almost not learning, random8 not learning

# Run 4
Rosenbrock (100,1)
Finding: vracer learning, slight improvement of rl-cmaes compared with plain cmeas on rosenrbock (8 and 32 samples)

# Run 5
new functions for random, taken from pk paper
Finding: agent is learning random functions
TODO: eval performance of all functions vs cmaes

# Run 6
reward based on distance to zero, running fsphere and random from pk
Finding: random seems to learn better than in Run 5
TODO: eval performance of all functions vs cmaes

# Run 7
added 3rd action to control dhat, running rosenbrock, reward from Run 4 (?)
Finding: agent does not learn

# Run 8
reward based on distance to zero (version 2), running fsphere and random from pk
Finding: 

# Run 9
same as 8, running batch script for rosenbrok and ackley
Finding:

# Ran 10
reward from Run 4, running random obj, increasing policy updates per action to 5, actions per policy up 0.2
Finding: agent learns, same as in Run 5

# Run 11
repeating random run from Run 10, 5M exp and with reward factor
Finding: agent learned, training converged after ~1M exp, evaluation showed that in 7/7 cases vracer learned a better version of cma-es

# Run 11 - LSTM
pop size 8, dim 2, lstm, depth 1, output layers 128
Finding: no advantage of using RNN

# Run 12
reward 12, running pop size 8, dim 2, vracer and lstm as in run 11
Finding: no advantage of using RNN

# Run 12
reward run 11, remove position of samples, only take evals as state
Finding: agent seems not to learn

Next steps, eval run 12 on objectives, based on that choose reward function.

# Run 13
batch run for random, dims 2..128, added rfacs for each dim
TODO: rerun batch form 8 & 64

# Run 15
run random with reward 4, but with new reward rescaling feature, dim 2, pop 8, dim 4 pop 16
Finding: reward 4 works well up to dim ~8

# Run 16
run random with reward 8, but with new reward rescaling feature, dim 2, pop 8, dim 4 pop 16
Finding: reward 8 does not work well


# Run 17
increasing NN size to 256, and then compare to Run 15 (dim 8 - 32)
Finding:

# Run 18
version 1, changed state to weighted cov of mu, NN size 128
Finding: it learns, up to dim 64 tested

# Run 19
version 1, changed state to weighted cov of mu, NN size 256
Finding:

# Run 20
version 1, added rosenbrock, NN 256

# Run 21
version 1, added ridge functions, NN 256

# Run 22
redo run 21, starting dum 8, I think I messed up sth before
