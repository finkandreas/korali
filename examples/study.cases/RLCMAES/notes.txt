# Run 5
standard setup
running 2,4,8 dimension w 1e7 experiences
Finding: learning stops for 2 dim after ~3e6 exp, no learning for 4 & 8 dim

# Run 6
added mu learning rate as action, removed diagonal cov states, scaling parameter samples by diagoanl sdev
running 2 and 4 dimension
Finding: no improvement detected, ill continue with this varyation as it seems to be more reasonable

# Run 1
Actions Between Policy Updates set to 1 (used to be 10!) Rerunning all objectives wo noise, lets see if the agent learns
Finding: agent learns, except for rosenbrock very flat. rosenbrock with params (100, 5) and spheres (5,5) same perf as cmaes. but worked well on booth, himmelblau and levi, next changing params of rosenbrock and retry (Run 2)
Todo: check runs of levi13 and ackley

# Run 2 (deleted)
using 3 actions, cs, cmu and csaddon. Rerunning rosenbrock and spheres wo noise, lets compare with run 0.
Finding: cov error!!!
Todo: investigate, follow-up

# Run 2
Same as Run 1, but changed params of rosenbrock (100, 3)
Finding: agent still learns flat w pop 8 and 32

# Run 3
Same as Run 2, but increasing population size to 32 s.t. variance of reward reduced  and potentially increases learning. running 'random' wo rosenbrock
Finding: learning seems to be similar, not much improvement when comparing with plain cma-es, random32 almost not learning, random8 not learning

# Run 4
Rosenbrock (100,1)
Finding: vracer learning, slight improvement of rl-cmaes compared with plain cmeas on rosenrbock (8 and 32 samples)

# Run 5
new functions for random, taken from pk paper
Finding: agent is learning random functions
TODO: eval performance of all functions vs cmaes

# Run 6
reward based on distance to zero, running fsphere and random from pk
Finding: random seems to learn better than in Run 5
TODO: eval performance of all functions vs cmaes

# Run 7
added 3rd action to control dhat, running rosenbrock, reward from Run 4 (?)
Finding: agent does not learn

# Run 8
reward based on distance to zero (version 2), running fsphere and random from pk
Finding: 

# Run 9
same as 8, running batch script for rosenbrok and ackley
Finding:

# Ran 10
reward from Run 4, running random obj, increasing policy updates per action to 5, actions per policy up 0.2
Finding:
