#include "engine.hpp"
#include "modules/solver/optimizer/ES/ES.hpp"
#include "sample/sample.hpp"

__startNamespace__;

void __className__::setInitialConfiguration()
{
  knlohmann::json problemConfig = (*_k)["Problem"];
  _variableCount = _k->_variables.size();

  // Establishing optimization goal
  _bestEverValue = -std::numeric_limits<double>::infinity();

  _previousBestEverValue = _bestEverValue;
  _previousBestValue = _bestEverValue;
  _currentBestValue = _bestEverValue;

  // Validating configuration params
  if (_populationSize == 0) KORALI_LOG_ERROR("Population Size must be larger 0 (is %zu)", _populationSize);
  if (_learningRate <= 0. || _learningRate > 1.) KORALI_LOG_ERROR("The Learning Rate must be in (0,1] (it is %lf).", _learningRate);

  // Allocating Memory
  _samplePopulation.resize(_populationSize);
  for (size_t i = 0; i < _populationSize; i++) _samplePopulation[i].resize(_variableCount);
  _randomVector.resize(_populationSize);
  for (size_t i = 0; i < _populationSize; i++) _randomVector[i].resize(_variableCount);

  _currentMean.resize(_variableCount);
  _previousMean.resize(_variableCount);
  _bestEverVariables.resize(_variableCount);
  _currentBestVariables.resize(_variableCount);
  _valueVector.resize(_populationSize);
  _weightVector.resize(_populationSize);

  _covarianceMatrix.resize(_variableCount * _variableCount);
  _sigmaMatrix.resize(_variableCount * _variableCount);
  
  // Version specific checks
  if (_version == "ES-EM")
  {
    _versionId = 0;
  }
  else if (_version == "ES-EM-SGA-v1")
  {
    _versionId = 1;
  }
  else if (_version == "ES-EM-SGA-v2")
  {
    _versionId = 2;
  }
  else if (_version == "ES-EM-SGA-v3")
  {
    if(_populationSize % 2 == 1) KORALI_LOG_ERROR("Mirrored Sampling can only be applied with an even Sample Population (is %zu)", _populationSize);
    _versionId = 3;
  }
  else if (_version == "ES-EM-C")
  {
    _versionId = 4;
  }
  else
    KORALI_LOG_ERROR("Version '%s' not recognized. Available options are 'ES-EM', 'ES-SGA-v1', 'ES-SGA-v2', 'ES-SGA-v3' and 'ES-EM-C'", _version);


  // Initializing variable defaults
  for (size_t i = 0; i < _variableCount; ++i)
  {
    if (std::isfinite(_k->_variables[i]->_initialValue) == false)
    {
      if (std::isfinite(_k->_variables[i]->_lowerBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
      if (std::isfinite(_k->_variables[i]->_upperBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
      _k->_variables[i]->_initialValue = (_k->_variables[i]->_upperBound + _k->_variables[i]->_lowerBound) * 0.5;
    }

    if (std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
    {
      if (std::isfinite(_k->_variables[i]->_lowerBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
      if (std::isfinite(_k->_variables[i]->_upperBound) == false) KORALI_LOG_ERROR("Initial Standard Deviation \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
      _k->_variables[i]->_initialStandardDeviation = (_k->_variables[i]->_upperBound - _k->_variables[i]->_lowerBound) * 0.3;
    }
  }
    
  // Set initial mean and covariance
  for(size_t d = 0; d < _variableCount; ++d) _currentMean[d] = _k->_variables[d]->_initialValue;
  for(size_t d = 0; d < _variableCount; ++d) _covarianceMatrix[d*_variableCount+d] = _k->_variables[d]->_initialStandardDeviation;
  
  _infeasibleSampleCount = 0;
}

void __className__::runGeneration()
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  // Initializing Sample Evaluation
  std::vector<Sample> samples(_populationSize);
  for (size_t i = 0; i < _populationSize; i++)
  {
    samples[i]["Module"] = "Problem";
    samples[i]["Operation"] = "Evaluate";
    samples[i]["Parameters"] = _samplePopulation[i];
    samples[i]["Sample Id"] = i;
    _modelEvaluationCount++;

    KORALI_START(samples[i]);
  }

  // Waiting for samples to finish
  KORALI_WAITALL(samples);

  // Gathering evaluations
  for (size_t i = 0; i < _populationSize; i++)
    _valueVector[i] = KORALI_GET(double, samples[i], "F(x)");

  updateDistribution();
}

void __className__::prepareGeneration()
{
  // Produce random numbers
  for (size_t i = 0; i < _populationSize; ++i)
  {
    if(_versionId == 2) // ES-SGA-v1
        if (i == 0) for(size_t d = 0; d < _variableCount; ++d) _randomVector[0][d] = 0.;
        else for(size_t d = 0; d < _variableCount; ++d) _randomVector[i][d] = _normalGenerator->getRandomNumber();

    else if(_versionId == 3) // ES-SGA-v3
        if (i % 2 == 0) for(size_t d = 0; d < _variableCount; ++d) _randomVector[i][d] = _normalGenerator->getRandomNumber();
        else for(size_t d = 0; d < _variableCount; ++d) _randomVector[i][d] = -_randomVector[i-1][d];

    else // ES-EM, ES-SGA-v0, ES-EM-C
        for(size_t d = 0; d < _variableCount; ++d) _randomVector[i][d] = _normalGenerator->getRandomNumber();
  }

  // Generate new population
  for (size_t i = 0; i < _populationSize; ++i)
  {
      sampleSingle(i, _randomVector[i]);
  }
}

void __className__::sampleSingle(size_t sampleIdx, const std::vector<double>& randomNumbers)
{
    for (size_t d = 0; d < _variableCount; ++d)
      for (size_t e = 0; e <= d; ++e) 
        _samplePopulation[sampleIdx][d] = _currentMean[d] + _sigmaMatrix[d*_variableCount+e] * randomNumbers[e];
}

void __className__::updateDistribution()
{
  // Update sorting index based on value vector
  sort_index(_valueVector, _sortingIndex, _populationSize);

  // Update current best value and variable
  _previousBestValue = _currentBestValue;
  _currentBestValue = _valueVector[_sortingIndex[0]];
  _currentBestVariables = _samplePopulation[_sortingIndex[0]];

  // Update best ever variables
  if (_currentBestValue > _bestEverValue || _k->_currentGeneration == 1)
  {
    _previousBestEverValue = _bestEverValue;
    _bestEverValue = _currentBestValue;
    _bestEverVariables = _currentBestVariables;
  }
 
  // Reset mean and covariance
  _previousMean = _currentMean;
  double sumOfValues = std::accumulate(_valueVector.begin(), _valueVector.end(), 0.);

  // Update mean and covariance
  if(_versionId == 0) // ES-EM
  {
    std::fill(_currentMean.begin(), _currentMean.end(), 0.f);
    std::fill(_covarianceMatrix.begin(), _covarianceMatrix.end(), 0.f);
 
    for(size_t i = 0; i < _populationSize; ++i)
    {
      _weightVector[i] = _valueVector[i]/sumOfValues;
      for(size_t d = 0; d < _variableCount; ++d)
      {
        _currentMean[d] += _weightVector[i]*_samplePopulation[i][d];
        for(size_t e = 0; e < d; ++e)
        {
          _covarianceMatrix[d*_variableCount+e] = _weightVector[i]*_samplePopulation[i][d]*_samplePopulation[i][e];
          _covarianceMatrix[e*_variableCount+d] = _covarianceMatrix[d*_variableCount+e];
        }
        _covarianceMatrix[d*_variableCount+d] = _weightVector[i]*_samplePopulation[i][d]*_samplePopulation[i][d];
      }
    }
  }
  else if(_versionId == 4) // ES-EM-C
  {
 
    for(size_t d = 0; d < _variableCount; ++d) _currentMean[d] *= (1.-_learningRate);
    for(size_t e = 0; e < _variableCount*_variableCount; ++e) _covarianceMatrix[e] *= (1.-_learningRate);
 
    for(size_t i = 0; i < _populationSize; ++i)
    {
      _weightVector[i] = _valueVector[i]/sumOfValues;
      for(size_t d = 0; d < _variableCount; ++d)
      {
        _currentMean[d] += _learningRate*_weightVector[i]*_samplePopulation[i][d];
        for(size_t e = 0; e < d; ++e)
          _covarianceMatrix[d*_variableCount+e] = _learningRate*_weightVector[i]*_samplePopulation[i][d]*_samplePopulation[i][e];
        
        _covarianceMatrix[d*_variableCount+d] = _learningRate*_weightVector[i]*_samplePopulation[i][d]*_samplePopulation[i][d];
      }
    }
 
    for(size_t d = 0; d < _variableCount; ++d)
    {
        for(size_t e = 0; e < d; ++e) 
        {
            _covarianceMatrix[d*_variableCount+e] -= _learningRate*_currentMean[d]*_currentMean[e];
            _covarianceMatrix[e*_variableCount+d] = _covarianceMatrix[d*_variableCount+e];
        }
        _covarianceMatrix[d*_variableCount+d] -= _learningRate*_currentMean[d]*_currentMean[d];
    }
 
  }
  else
  {

    size_t start;
    size_t incr;
    if(_versionId == 1) // ES-SGA-v1
    {
        for(size_t i = 0; i < _populationSize; ++i)
            _weightVector[i] = _learningRate*_valueVector[i]/sumOfValues;

        start = 0;
        incr = 1;
    }
    else if(_versionId == 2) // ES-SGA-v2
    {
        for(size_t i = 1; i < _populationSize; ++i)
            _weightVector[i] = _learningRate*_valueVector[i]-_valueVector[0];

        start = 1;
        incr = 1;
    }
    else if(_versionId == 3) // ES-SGA-v3
    {
        for(size_t i = 0; i < _populationSize; i+=2)
            _weightVector[i] = _learningRate*0.5*(_valueVector[i]-_valueVector[i+1]);

        start = 0;
        incr = 2;
    }

    for(size_t i = start; i < _populationSize; i+=incr)
    {
        for(size_t d = 0; d < _variableCount; ++d)
            for(size_t e = 0; e < d; ++e) 
                _currentMean[d] += _weightVector[i]*_sigmaMatrix[d*_variableCount+e]*_randomVector[i][e];

        for(size_t d = 0; d < _variableCount; ++d)
        {
            for(size_t e = 0; e <= d; ++e)
            {
                for(size_t f = 0; f < _variableCount; ++f) 
                    if(e == f)
                        _covarianceMatrix[d*_variableCount+e] += _weightVector[i]*_covarianceMatrix[d*_variableCount+f]*_randomVector[i][e]*_randomVector[i][f] - 1.;
                    else
                        _covarianceMatrix[d*_variableCount+e] += _weightVector[i]*_covarianceMatrix[d*_variableCount+f]*_randomVector[i][e]*_randomVector[i][f];
                _covarianceMatrix[d*_variableCount+e] = _covarianceMatrix[e*_variableCount+d];
            }
        }
    }
  }
 
}

void __className__::sort_index(const std::vector<double> &vec, std::vector<size_t> &sortingIndex, size_t N) const
{
  // initialize original sortingIndex locations
  std::iota(std::begin(sortingIndex), std::begin(sortingIndex) + N, (size_t)0);

  // sort indexes based on comparing values in vec
  std::sort(std::begin(sortingIndex), std::begin(sortingIndex) + N, [vec](size_t i1, size_t i2) { return vec[i1] > vec[i2]; });
}

void __className__::printGenerationBefore() { return; }

void __className__::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Current Function Value: Max = %+6.3e - Best = %+6.3e\n", _currentBestValue, _bestEverValue);
  _k->_logger->logInfo("Normal", "Diagonal Covariance:    Min = %+6.3e -  Max = %+6.3e\n", _minimumDiagonalCovarianceMatrixElement, _maximumDiagonalCovarianceMatrixElement);

  _k->_logger->logInfo("Detailed", "Variable = (MeanX, BestX):\n");
  for (size_t d = 0; d < _variableCount; d++) _k->_logger->logData("Detailed", "         %s = (%+6.3e, %+6.3e)\n", _k->_variables[d]->_name.c_str(), _currentMean[d], _bestEverVariables[d]);

  _k->_logger->logInfo("Detailed", "Covariance Matrix:\n");
  for (size_t d = 0; d < _variableCount; d++)
  {
    for (size_t e = 0; e <= d; e++) _k->_logger->logData("Detailed", "   %+6.3e  ", _covarianceMatrix[d * _variableCount + e]);
    _k->_logger->logInfo("Detailed", "\n");
  }

  _k->_logger->logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

void __className__::finalize()
{
  // Updating Results
  (*_k)["Results"]["Best Sample"]["F(x)"] = _bestEverValue;
  (*_k)["Results"]["Best Sample"]["Parameters"] = _bestEverVariables;

  _k->_logger->logInfo("Minimal", "Optimum found at:\n");
  for (size_t d = 0; d < _variableCount; ++d) _k->_logger->logData("Minimal", "         %s = %+6.3e\n", _k->_variables[d]->_name.c_str(), _bestEverVariables[d]);
  _k->_logger->logInfo("Minimal", "Optimum found: %e\n", _bestEverValue);
  _k->_logger->logInfo("Minimal", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

__moduleAutoCode__;

__endNamespace__;
